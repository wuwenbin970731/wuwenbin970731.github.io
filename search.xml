<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Mamba系列日积月累（一）：状态空间模型SSM的离散化过程推导</title>
      <link href="/posts/c8aa64ed/"/>
      <url>/posts/c8aa64ed/</url>
      
        <content type="html"><![CDATA[<p>最近Mamba系列（<a href="https://arxiv.org/abs/2312.00752">Mamba</a>、<a href="https://arxiv.org/abs/2401.10166">VMamba</a>、<a href="https://arxiv.org/abs/2401.09417">Vision Mamba</a>）比较火，在同样具备高效长距离建模能力的情况下，相较于Transformer的平方级计算复杂度，Mamba架构则是线性级计算复杂度，并且推理速度更快。</p><p>秉承着<s>公众号科研的思路</s>扩展视野的思路，笔者觉得需要学习一下相关内容，于是挑选了目前较新的<a href="https://arxiv.org/abs/2401.09417">Vision Mamba</a>论文，准备开始学习。由于缺乏之前的基础知识储备，Preliminaries里面的状态空间模型及其离散化过程直接给我干蒙，想着不能出师未捷身先死，于是决定搜索相关知识，把这个过程搞清楚弄明白，如果内容存在错误，希望大家能给出指导以进行纠错。</p><h2 id="1-背景基础知识">1. 背景基础知识</h2><h3 id="1-1-什么是状态空间模型（State-Space-Model，SSM）？">1.1 什么是状态空间模型（State Space Model，SSM）？</h3><p>状态空间模型（State Space Model，简称SSM）是一种数学模型，用于描述和分析动态系统的行为。这种模型在多个领域都有应用，包括控制理论、信号处理、经济学和机器学习等。在深度学习领域，状态空间模型被用来处理序列数据，如时间序列分析、自然语言处理（NLP）和视频理解等。通过将序列数据映射到状态空间，可以更好地捕捉数据中的长期依赖关系。</p><p>状态空间模型的核心思想是将系统的当前状态$x(t) \in \mathbb{R}^n$（state）与输入$u(t) \in \mathbb{R}^p$（input）和输出$y(t) \in \mathbb{R}^q$​​（output）之间的关系用一组方程来表示：<br>$$<br>\begin{aligned}<br>&amp; \dot{x}(t)=A(t) x(t)+B(t) u(t) \<br>&amp; y(t)=C(t) x(t)+D(t) u(t)<br>\end{aligned}<br>\tag{1}<br>$$</p><ol><li><strong>状态方程（State Equation）</strong>：描述系统状态随时间的演变。状态方程通常包含当前状态和输入，以及可能的系统参数。数学上，状态方程可以表示为： $\dot{x}(t)=A(t) x(t)+B(t) u(t)$， 其中，$x(t)$是在时间步 $t$ 的系统状态，$\dot{x}(t)$是状态向量$x(t)$关于时间 $t$的导数，$u(t)$ 是在时间步 $t$的输入，$A(t)$是状态转移矩阵，$\operatorname{dim}[A(\cdot)]=n \times n$，$B$ 是输入矩阵，$\operatorname{dim}[B(\cdot)]=n \times p$。</li><li><strong>观测方程（Observation Equation）</strong>：描述系统输出与状态之间的关系。观测方程允许我们从系统的输出中观察到系统的状态。数学上，观测方程可以表示为： $y(t)=C(t) x(t)+D(t) u(t)$ 其中，$y(t)$ 是在时间步 $t$ 的系统输出，$C(t)$是观测矩阵，$\operatorname{dim}[C(\cdot)]=q \times n$，$D(t)$ 是前馈矩阵，$\operatorname{dim}[D(\cdot)]=q \times p$。</li></ol><p>当式(1)中的所有矩阵均随着时间$t$而变化时，此时所表示的线性时变系统，而当所有矩阵都不随时间$t$​变化时，此时表示的是线性非时变系统，在Mamba系列中，实际上是线性非时变系统：<br>$$<br>\begin{aligned}<br>&amp; \dot{x}(t)=A x(t)+B u(t) \<br>&amp; y(t)=C x(t)+D u(t)<br>\end{aligned}<br>\tag{2}<br>$$</p><h3 id="1-2-什么是离散化（Discretization）？">1.2 什么是离散化（Discretization）？</h3><p>离散化（Discretization）是将连续的数学对象或过程转换为离散形式的过程。在不同的领域中，离散化有着不同的应用和含义，但核心思想是一致的：将连续的变量或函数映射到有限的、离散的集合中。这个过程在数学、工程、计算机科学和许多其他领域中都非常常见。</p><h3 id="1-3-为什么需要离散化？">1.3 为什么需要离散化？</h3><p>SSM作为一个连续时间系统，其难以直接集成到现代深度学习算法中：</p><ul><li><strong>计算效率</strong>：现代深度学习框架和硬件通常是基于离散时间操作而设计的，对SSM进行离散化后，才能将其转化为可以在这些框架和硬件上高效运行的模型。</li><li><strong>训练算法</strong>：大多数深度学习训练算法，如梯度下降和反向传播，都是为离散时间模型设计的。离散化使得这些算法可以直接应用于状态空间模型，简化了训练过程。</li><li><strong>实际应用</strong>：在许多实际应用中，数据是离散的，如文本数据（单词序列）、时间序列数据（股票价格、传感器读数）等。离散时间模型更自然地与这些数据格式相匹配。</li><li><strong>模型复杂度</strong>：离散化过程可以通过选择合适的时间步长 $T$ 来控制模型的复杂度。较小的时间步长可以提供更精细的控制，但计算成本更高；较大的时间步长可以减少计算量，但可能牺牲一些精度。</li></ul><h2 id="2-SSM离散化过程推导">2. SSM离散化过程推导</h2><p>这里再贴上状态方程公式<br>$$<br>\dot{x}(t)=A x(t)+B u(t) \<br>\tag{3}<br>$$<br>为了进行离散化，我们首先要对状态方程(3)进行积分。</p><h3 id="2-1-为什么在离散化过程中要先进行积分？">2.1 为什么在离散化过程中要先进行积分？</h3><p>在离散化连续状态方程的过程中，积分是一个关键步骤，因为它涉及到状态变量随时间的累积效应，我们需要考虑在每个离散时间步长内状态变量是如何累积变化的。</p><p>在离散时间系统中，我们不能直接处理导数，因为离散时间点上没有导数的概念。相反，我们需要考虑在每个时间步长内状态变量的累积变化。这可以通过对连续时间积分进行离散化来实现，即将连续时间的积分转换为离散时间的求和。</p><p>在实际的数值模拟中，我们通常使用数值积分方法（如梯形法则、辛普森法则等）来近似连续时间积分。这些方法允许我们在离散时间点上近似连续时间的累积效应，从而得到离散时间状态方程。这个转换过程涉及到将连续时间的导数项替换为离散时间的差分项，这通常涉及到<strong>指数函数</strong>和采样间隔 $T$​ 的计算。</p><h3 id="2-2-为什么不直接对-dot-x-t-进行积分？">2.2 为什么不直接对$\dot{x}(t)$进行积分？</h3><p>在式(3)中，假设我们直接对$\dot{x}(t)$进行积分的话，结果如下：<br>$$<br>x(t)=x(0)+\int_0^t(A x(\tau)+B u(\tau)) d \tau<br>\tag{4}<br>$$<br>此时，积分项中会包含$x(\tau)$项本身，由于我们是离散系统，我们是无法获取在一个连续的时刻（$0\rightarrow t$）内所有的$x(\tau)$值的，因此无法完成该积分结果的计算。</p><p>对于离散系统来说，我们希望将公式(4)这个积分表达式转变为以下形式：<br>$$<br>x(k+1)=x(k)+\sum_{i=0}^k(A x(i)+B u(i)) \Delta t<br>\tag{5}<br>$$<br>这个形式要求我们对公式(3)进行一些改造，目标是消除$\dot{x}(t)$表达式中的$x(t)$本身。</p><h3 id="2-2-状态方程的改造以及-alpha-t-的设计">2.2 状态方程的改造以及$\alpha(t)$的设计</h3><p>为了消除$\dot{x}(t)$表达式中的$x(t)$本身，我们通常会构造一个新的函数$\alpha(t)x(t)$，通过对这个新函数进行求导，来简化相应的导数项。</p><p>我们对$\alpha(t)x(t)$​进行求导</p><p>$$<br>\frac{d}{d t}[\alpha(t) x(t)]=\alpha(t) \dot{x}(t)+x(t) \frac{d \alpha(t)}{d t}<br>\tag{6}<br>$$<br>我们将公式(3)代入到公式(6)中，替换$\dot{x}(t)$：</p><p>$$<br>\frac{d}{d t}[\alpha(t) x(t)]=\alpha(t) (A x(t)+B u(t))+x(t) \frac{d \alpha(t)}{d t}<br>\tag{7}<br>$$<br>我们进一步对公式(7)进行改写，合并$x(t)$的相关系数：</p><p>$$<br>\frac{d}{d t}[\alpha(t) x(t)]=(A\alpha(t) + \frac{d \alpha(t)}{d t})x(t)+B \alpha(t) u(t)<br>\tag{8}<br>$$<br>由于我们的目的是消除导数项中的$x(t)$，因此，我们令$x(t)$的系数项为0即可：<br>$$<br>A\alpha(t) + \frac{d \alpha(t)}{d t} = 0<br>\tag{9}<br>$$<br>此时，我们可以得到$\alpha(t)$的表达式：<br>$$<br>\alpha(t)=e^{-At}<br>\tag{10}<br>$$<br>将$\alpha(t)$的表达式代入公式(8)可以得到：<br>$$<br>\frac{d}{d t}[e^{-At} x(t)]=B e^{-At} u(t)<br>\tag{11}<br>$$<br>这时我们已经完成了在导数项中消除$x(t)$的目标，对$e^{-At}x(t)$进行积分：<br>$$<br>e^{-At}x(t)=x(0)+\int_0^t e^{-A\tau} B u(\tau) d \tau<br>\tag{12}<br>$$<br>对公式(12)进行整理：</p><p>$$<br>x(t)=e^{At}x(0)+\int_0^t e^{A(t-\tau)} B u(\tau) d \tau<br>\tag{13}<br>$$</p><h3 id="2-3-离散时间积分近似">2.3 离散时间积分近似</h3><h3 id="2-3-状态方程的离散化">2.3 状态方程的离散化</h3><p>在离散系统中，我们需要将公式(13)转化为离散形式，大致步骤如下：</p><ul><li><p><strong>参数定义</strong>：采样时刻$t_k$和$t_{k+1}$，其中$k$是采样索引，$T$是采样间隔，即$T=t_{k+1}-t_k$​</p></li><li><p><strong>积分区间离散化</strong>：在连续时间积分中，我们通常有一个积分区间，例如从$t$ 到 $t+\triangle{t}$。在离散时间系统中，我们需要将这个区间划分为$k$ 个等长的子区间，每个子区间的长度为$T$​​。</p><p>在某个子区间内，公式(13)的形式变为：<br>$$<br>x(t_{k+1})=e^{A(t_{k+1}-t_k)}x(t_{k})+\int_{t_{k}}^{t_{k+1}} e^{A(t_{k+1}-\tau)} B u(\tau) d \tau<br>\tag{14}<br>$$</p></li><li><p><strong>近似积分</strong>：对于每个子区间来说，考虑使用数值积分方法来近似积分，这里考虑对$u(t)$应用零阶保持法，即假设$u(t)$在采样时刻$t_k$和$t_{k+1}$之间是恒定的，此时，我们可以将$u(t)$当做常数项从积分项中取出：<br>$$<br>\int_{t_{k}}^{t_{k+1}} e^{A(t-\tau)} B u(\tau) d \tau = \int_{t_{k}}^{t_{k+1}} e^{A(t_{k+1}-\tau)} d \tau B u(t_k)<br>\tag{15}<br>$$</p></li><li><p><strong>离散时间状态方程构建</strong>：将公式(15)的积分结果代入到公式(14)中，同时使用$T=t_{k+1}-t_k$​进行化简，我们可以得到：<br>$$<br>x(t_{k+1})=e^{AT}x(t_{k})+\int_{t_{k}}^{t_{k+1}} e^{A(t_{k+1}-\tau)} d \tau Bu\left(t_k\right)<br>\tag{16}<br>$$<br>引入新变量$\lambda=t_{k+1}-\tau$，对原积分进行简化得到：<br>$$<br>x(t_{k+1})=e^{AT}x(t_{k})+Bu\left(t_k\right)\int_{0}^{T} e^{A\tau} d \tau<br>\tag{17}<br>$$<br>这里涉及到矩阵作为指数的积分，这个部分我是查阅一些资料得到的结果：<br>$$<br>\int_{0}^{T} e^{A\tau} d \tau=A^{-1}(e^{AT}- I)<br>\tag{18}<br>$$<br>最终我们得到了离散时间状态方程：<br>$$<br>x(t_{k+1})=e^{AT}x(t_{k})+(e^{AT}- I)A^{-1}B u\left(t_k\right)<br>\tag{19}<br>$$</p></li></ul><p>至此，我们完成了论文中公式（1）：</p><img src="http://rainbowu-images.oss-cn-beijing.aliyuncs.com/2024/01/29/image20240129012412998.png" alt="Vision Mamba论文中的SSM公式" style="zoom: 50%;" /><p>到公式(2)的推导：</p><img src="http://rainbowu-images.oss-cn-beijing.aliyuncs.com/2024/01/29/image20240129012440256.png" alt="Vision Mamba论文中离散化结果" style="zoom:50%;" />]]></content>
      
      
      <categories>
          
          <category> 论文解读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数学 </tag>
            
            <tag> Mamba系列 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>关于我的个人博客</title>
      <link href="/posts/93968806/"/>
      <url>/posts/93968806/</url>
      
        <content type="html"><![CDATA[<p>想想大概是在2020年，那个时候开始学习深度学习相关的内容，并且去了一个公司实习，那段时间每天都在学习很多新知识，非常充实。想着对学习的一些内容进行记录，于是我便陆陆续续开始在<a href="https://blog.csdn.net/weixin_39778049?spm=1011.2415.3001.5343">CSDN</a>、<a href="https://www.zhihu.com/people/RainboWu731">知乎</a>上写博客，一开始只是想着进行简单记录，后来发现真的会有人来看你写的内容，甚至给你点赞，一打开知乎，好多红心的那种感觉非常让人上瘾，那个时间应该是我最喜欢写博客的日子了。</p><p>后来由于找工作、忙毕业等事情，便基本上不再去写博客了，也疏于了对于博客的管理，例如对于大家提问的回答等。工作之后缺乏动力和精力来写东西了，晚上回来只想歇着，周末也只想歇着，感觉人已经废了一大半。</p><p>2023年末的时候看到苏剑林大佬发了一篇知乎，说他手搓了一个刷Arxiv的<a href="https://papers.cool/">网站</a>，更进一步地，我去看了一下大佬的个人博客（虽然之前也知道苏神的博客，但一直没有细看过），发现内容之丰富令我咋舌，而且每篇文章内容质量都非常高。这个高质量的个人博客燃起了我搭建自己个人博客的又一次冲动。</p><p>于是，经过查找相关资料（感谢大家的热心分享），终于有了现在的这个<a href="https://wuwenbinsights.com/">博客</a>，现在这个博客还比较简陋，后续会在持续进行优化。这个博客将用来记录一些论文的阅读笔记、读书笔记、个人感悟以及其他任何我觉得值得记录的内容，希望这个博客可以让我多写、多想。</p>]]></content>
      
      
      <categories>
          
          <category> 个人 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 感悟 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
